{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a386f7a",
   "metadata": {},
   "source": [
    "# In this project we will use OpenCV to capture and record videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c14ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not yet installed openCV, type the command : pip install opencv-python.\n",
    "\n",
    "# Let’s dive into the main program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365175e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "   \n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3,5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),5)\n",
    "\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    if(cv2.waitKey(1)==ord('e')):           # Press 'e' to stop the camera.\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7769f",
   "metadata": {},
   "source": [
    "It can also detect eyes just include Haar cascade eyes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8596c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import cv2\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3,5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),5)\n",
    "\n",
    "        roi_gray = gray[y:y+w,x:x+w]\n",
    "        roi_color = frame[y:y+h,x:x+w]\n",
    "\n",
    "        eyes= eye_cascade.detectMultiScale(roi_gray,1.3,5)\n",
    "\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),5)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    if(cv2.waitKey(1)==ord('q')):            # Press 'q' to stop the camera.\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5858938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To detect eyes sharp and clear, what has been done here is, region_of_interest, ROI variable has made so that only the face \n",
    "# region is passed through eye_cascade.detectMultiScale and then the rectangles for eyes have drawn on the actual frame image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761f43e",
   "metadata": {},
   "source": [
    "Let’s detect eyes and faces from an mp4 video. Include mp4 instead of camera id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fba37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use Jupyter notebook, you must upload the mp4 video in Jupyter home page before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "417e164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import cv2\n",
    "\n",
    "cap=cv2.VideoCapture('video..mp4')\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3,5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),5)\n",
    "\n",
    "        roi_gray = gray[y:y+w,x:x+w]\n",
    "        roi_color = frame[y:y+h,x:x+w]\n",
    "\n",
    "        eyes= eye_cascade.detectMultiScale(roi_gray,1.3,5)\n",
    "\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),5)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    if(cv2.waitKey(1)==ord('q')):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# The blue rectangle denotes the faces and the green one denotes the eyes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06297a3e",
   "metadata": {},
   "source": [
    "Explanation of the main lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9268701",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# VideoCapture() takes parameter as the id of the camera through which the object has to capture. \n",
    "# Here 0 stands for first camera or primary/main camera, if your system has a lot of cameras, you can specify their \n",
    "# number as 1, 2, 3, etc, depends on which the number of cameras you want to turn on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e510661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, as the cap variable is storing all the frames that have been captured, it is important to set an infinite loop and \n",
    "# wait for the keypress by the user to break the loop or close the camera. Also, it’s important to read the frame and showcase \n",
    "# it using imshow()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Frame', frame)\n",
    "    if(cv2.waitKey(1) == ord('e')):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cap.destroyAllWind\n",
    "\n",
    "\n",
    "# cap.read() returns the frame captured and flag that specifies everything is working error-free.\n",
    "# The camera must be released so that other applications can use it if it’s required, hence cap.release().\n",
    "# To break the loop, i.e. to close the window/camera, it’s important to specify an instruction that will make the user \n",
    "# escape from the window. Hence waitKey() is compared with the ord() of the desired characteristics that must be press to escape \n",
    "# In this case, the way to escape is by pressing the key ‘e’.\n",
    "\n",
    "# Note: ord(char) takes a char as a parameter and returns its ASCII value"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48b8cd5f",
   "metadata": {},
   "source": [
    "For detection of objects from the frame, it must be converted to grayscale as many operations in OpenCV are done through grayscale.\n",
    "\n",
    "The method used for conversion is cv2.cvtColor()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15cb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# cvtColor() takes two compulsory parameters\n",
    "# source and color space conversion code -> specifies from which color system to which the source has to convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, if we display the gray instead of the frame, we can see the gray camera mode.\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('Frame', frame)\n",
    "    if(cv2.waitKey(1) == ord('e')):\n",
    "        break\n",
    "        \n",
    "cap.release(1)\n",
    "cap.destroyAllWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71adcc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After face detection, the program must specify a rectangle or circle or eclipse any shape you desire to differ face from \n",
    "# other objects.\n",
    "\n",
    "# Let’s see how to draw shapes with openCV in video capture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8438bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.rectangle(img, (300, 300), (200, 200), (134, 56, 132), 6)\n",
    "\n",
    "\n",
    "# cv2.rectangle() draws rectangle over the image/ video capture i.e. over the source and takes parameters as :\n",
    "# source, starting point co-ordinates, ending point co-ordinates, color value (RGB), thickness in px (-1 to fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for face cascade, path where it is stored specific classifier\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How will we detect face then? With the help of the above Haar Cascade path and using detectMultiScale() we can store \n",
    "# coordinates of face/faces in a tuple.\n",
    "\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6183e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detectMultiScale() takes 3 parameters:\n",
    "\n",
    "# 1. image/source\n",
    "\n",
    "# 2. scale factor -> specifies how much the image size is reduced at each image scale.\n",
    "\n",
    "# 3. min Neighbors ->Parameter specifying how many neighbors each candidate rectangle should have to retain it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
